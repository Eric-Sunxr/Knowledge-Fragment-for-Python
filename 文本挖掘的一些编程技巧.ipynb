{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 如何在nltk中下载 \"stopwords（停用词）\" and \"punkt（标点符号）\" 这两个工具包？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Eric/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading packages and importing\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')#标点符号\n",
    "nltk.download('stop')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 如何在spacy导入语言模型（language model） ？\n",
    "\n",
    "Q. 导入spacy库并加载'en_core_web_sm'模型以支持英语语言模型。加载'xx_ent_wiki_sm'以支持多语言。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x182b00e64c8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and load model\n",
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "nlp\n",
    "# More models here: https://spacy.io/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 如何对给定的文本进行词条化（tokenization）?\n",
    "\n",
    "Q. 对给定文本词条化后的词条（token）进行打印。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Eric\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.981 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['通过', '对', '全网', '主流', '媒体', '及', '社交', '媒体', '平台', '进行', '实时', '数据', '抓取', '和', '深度', '处理', '，', '可以', '帮助', '政府', '/', '企业', '及时', '、', '全面', '、', '精准', '地', '从', '海量', '的', '数据', '中', '了解', '公众', '态度', '、', '掌控', '舆论', '动向', '、', '聆听', '用户', '声音', '、', '洞察', '行业', '变化', '。']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "text ='''通过对全网主流媒体及社交媒体平台进行实时数据抓取和深度处理，可以帮助政府/企业及时、全面、精准地从海量的数据中了解公众态度、掌控舆论动向、聆听用户声音、洞察行业变化。'''\n",
    "text_segment=jieba.lcut(text)\n",
    "print(text_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 如何对给定的文档进行语句划分?\n",
    "\n",
    "Q. 打印经过语句分割后的文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyplt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d6642b9ab164>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyplt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentenceSplitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'''社会化聆听可以7*24小时全天侯对全网进行实时监测，采用文本挖掘技术对民众意见反馈、领导发言、政策文件进行监测分析。通过这些先进的nlp技术的处理，可以帮助政府及时的了解社会各阶层民众对社会现状和发展所持有的情绪、态度、看法、意见和行为倾向 。最终实现积极主动的响应处理措施和方案，对于互联网上一些错误的、失实的舆论做出正确的引导作用，控制舆论发展方向。'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSentenceSplitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyplt'"
     ]
    }
   ],
   "source": [
    "from pyplt import SentenceSplitter\n",
    "docs = '''社会化聆听可以7*24小时全天侯对全网进行实时监测，采用文本挖掘技术对民众意见反馈、领导发言、政策文件进行监测分析。通过这些先进的nlp技术的处理，可以帮助政府及时的了解社会各阶层民众对社会现状和发展所持有的情绪、态度、看法、意见和行为倾向 。最终实现积极主动的响应处理措施和方案，对于互联网上一些错误的、失实的舆论做出正确的引导作用，控制舆论发展方向。'''\n",
    "sentences=SentenceSplitter.split(docs)\n",
    "print('\\n'.join(list(sentences)))\n",
    "\n",
    "#会自动将这个文本分段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 如何利用 transformers 对文本进行词条化 ?\n",
    "\n",
    "Q. 使用Huggingface的 transformers库直接对文本进行词条化处理，不需要分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2190, 1062, 1385, 1501, 4277, 6822, 6121, 6566, 7481, 5644, 2658, 2141, 3198, 4664, 3844, 8024, 752, 816, 4028, 1359, 6633, 1232, 7564, 3844, 8024, 7564, 6356, 2141, 3198, 6239, 6809, 8024, 2376, 1221, 1062, 1385, 2356, 1767, 1350, 1501, 4277, 6956, 7305, 5018, 671, 3198, 7313, 1355, 4385, 6566, 7481, 5644, 2658, 8024, 1350, 3198, 2418, 2190, 1314, 3322, 1062, 1068, 8024, 2971, 1169, 5644, 6389, 6624, 1403, 8024, 7344, 3632, 1501, 4277, 1358, 2938, 511, 102]\n",
      "[CLS] 对 公 司 品 牌 进 行 负 面 舆 情 实 时 监 测 ， 事 件 演 变 趋 势 预 测 ， 预 警 实 时 触 达 ， 帮 助 公 司 市 场 及 品 牌 部 门 第 一 时 间 发 现 负 面 舆 情 ， 及 时 应 对 危 机 公 关 ， 控 制 舆 论 走 向 ， 防 止 品 牌 受 损 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Import tokenizer from transfromers\n",
    "from transformers import AutoTokenizer\n",
    "text = '''对公司品牌进行负面舆情实时监测，事件演变趋势预测，预警实时触达，\n",
    "        帮助公司市场及品牌部门第一时间发现负面舆情，及时应对危机公关，控制舆论走向，\n",
    "     防止品牌受损。'''\n",
    "# Initialize the tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "# 使用tokenizer对文本进行编码\n",
    "inputs=tokenizer.encode(text)\n",
    "print(inputs)\n",
    "# 使用tokenizer对文本编码进行解码\n",
    "outputs=tokenizer.decode(inputs)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 如何在对文本词条化时将停用词（stopwords）用作短语区隔符号 \n",
    "\n",
    "Q. 有时把停用词用作区隔符号，将会保留有意义的短语（ meaningful phrases），这对于很多场景，如文本分类、主题建模和文本聚类都很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELIM文本挖掘DELIMDELIM功能DELIM达观数据DELIM多年DELIM自然语言处理技术经验DELIM掌握从词语短串到篇章分析DELIM层面DELIM分析技术DELIMDELIM基础DELIM提供DELIM文本挖掘功能DELIMDELIMDELIM 涉黄涉政检测DELIMDELIM文本内容DELIM涉黄涉政检测DELIM满足相应政策要求DELIMDELIMDELIM 垃圾评论过滤DELIM在论坛发言DELIM用户评论中DELIM过滤文本DELIM垃圾广告DELIM提升文本总体质量DELIMDELIMDELIM 情感分析DELIMDELIM用户评论DELIM文本内容DELIM情感分析DELIM指导决策与运营DELIMDELIMDELIM 自动标签提取DELIM自动提取文本重要内容生成关键性标签DELIMDELIM基础DELIM拓展DELIM功能形式DELIMDELIMDELIM 文本自动分类DELIMDELIMDELIM文本内容DELIM分析DELIM给出文本所属DELIM类别和置信度DELIM支持二级分类DELIMDELIM正常政治言论DELIMDELIM被过滤掉DELIMDELIMDELIMDELIMDELIMDELIM达观DELIM涉政内容DELIM返回DELIM“反动”权值DELIM取值范围0到1DELIMDELIM涉政内容DELIM反动权值接近“1”时DELIM文本DELIM反动倾向DELIM高DELIM根据客户要求DELIM直接过滤掉DELIMDELIM反动权值接近“0”时DELIMDELIM文本为正常政治言论DELIM几率DELIM非常高DELIM客户DELIMDELIM反动权值控制审核松紧程度DELIMDELIM黄反内容DELIM垃圾广告形式多样DELIM处理DELIMDELIM传统DELIM方法DELIMDELIM是DELIM配词典DELIM方式来解决DELIMDELIMDELIM方法DELIM变形文本时命中率DELIM低DELIM造成严重DELIM漏盘DELIM而且需要人工DELIM断更新词典DELIM效率DELIM低DELIMDELIM达观数据DELIM机器学习DELIM方法智能识别DELIM种变形变换DELIM内容DELIM同时根据最新DELIM样本数据实时更新运算模型DELIM自动学习更新DELIM保证检测DELIM效果DELIMDELIM实时DELIM弹幕能够DELIM处理DELIMDELIMDELIMDELIMDELIM达观数据文本挖掘系统支持高并发大数据量实时处理DELIM完全DELIM支持实时弹幕DELIM处理DELIM实现DELIM弹幕文本DELIM筛除涉黄DELIM涉政DELIM垃圾评论DELIM广告内容DELIMDELIM检测DELIMDELIM标签自动提取DELIM非热门行业适用DELIMDELIMDELIM达观标签自动提取功能DELIM利用行业数据DELIM模型训练和调整DELIM在接入DELIM非热门行业服务DELIMDELIMDELIMDELIMDELIM行业DELIM规范文本DELIM训练样本DELIM模型训练DELIM新DELIM模型更新DELIMDELIM适应此行业DELIM个性化需求DELIM而且在后期应用DELIM过程模型DELIMDELIM断DELIM更新迭代保证提取DELIM结果与行业DELIM发展保持同步DELIM\n",
      "['', '文本挖掘', '', '功能', '达观数据', '多年', '自然语言处理技术经验', '掌握从词语短串到篇章分析', '层面', '分析技术', '', '基础', '提供', '文本挖掘功能', '', '', '涉黄涉政检测', '', '文本内容', '涉黄涉政检测', '满足相应政策要求', '', '', '垃圾评论过滤', '在论坛发言', '用户评论中', '过滤文本', '垃圾广告', '提升文本总体质量', '', '', '情感分析', '', '用户评论', '文本内容', '情感分析', '指导决策与运营', '', '', '自动标签提取', '自动提取文本重要内容生成关键性标签', '', '基础', '拓展', '功能形式', '', '', '文本自动分类', '', '', '文本内容', '分析', '给出文本所属', '类别和置信度', '支持二级分类', '', '正常政治言论', '', '被过滤掉', '', '', '', '', '', '达观', '涉政内容', '返回', '“反动”权值', '取值范围0到1', '', '涉政内容', '反动权值接近“1”时', '文本', '反动倾向', '高', '根据客户要求', '直接过滤掉', '', '反动权值接近“0”时', '', '文本为正常政治言论', '几率', '非常高', '客户', '', '反动权值控制审核松紧程度', '', '黄反内容', '垃圾广告形式多样', '处理', '', '传统', '方法', '', '是', '配词典', '方式来解决', '', '', '方法', '变形文本时命中率', '低', '造成严重', '漏盘', '而且需要人工', '断更新词典', '效率', '低', '', '达观数据', '机器学习', '方法智能识别', '种变形变换', '内容', '同时根据最新', '样本数据实时更新运算模型', '自动学习更新', '保证检测', '效果', '', '实时', '弹幕能够', '处理', '', '', '', '', '达观数据文本挖掘系统支持高并发大数据量实时处理', '完全', '支持实时弹幕', '处理', '实现', '弹幕文本', '筛除涉黄', '涉政', '垃圾评论', '广告内容', '', '检测', '', '标签自动提取', '非热门行业适用', '', '', '达观标签自动提取功能', '利用行业数据', '模型训练和调整', '在接入', '非热门行业服务', '', '', '', '', '行业', '规范文本', '训练样本', '模型训练', '新', '模型更新', '', '适应此行业', '个性化需求', '而且在后期应用', '过程模型', '', '断', '更新迭代保证提取', '结果与行业', '发展保持同步', '']\n",
      "['文本挖掘', '功能', '达观数据', '多年', '自然语言处理技术经验', '掌握从词语短串到篇章分析', '层面', '分析技术', '基础', '提供', '文本挖掘功能', '涉黄涉政检测', '文本内容', '涉黄涉政检测', '满足相应政策要求', '垃圾评论过滤', '在论坛发言', '用户评论中', '过滤文本', '垃圾广告', '提升文本总体质量', '情感分析', '用户评论', '文本内容', '情感分析', '指导决策与运营', '自动标签提取', '自动提取文本重要内容生成关键性标签', '基础', '拓展', '功能形式', '文本自动分类', '文本内容', '分析', '给出文本所属', '类别和置信度', '支持二级分类', '正常政治言论', '被过滤掉', '达观', '涉政内容', '返回', '“反动”权值', '取值范围0到1', '涉政内容', '反动权值接近“1”时', '文本', '反动倾向', '根据客户要求', '直接过滤掉', '反动权值接近“0”时', '文本为正常政治言论', '几率', '非常高', '客户', '反动权值控制审核松紧程度', '黄反内容', '垃圾广告形式多样', '处理', '传统', '方法', '配词典', '方式来解决', '方法', '变形文本时命中率', '造成严重', '漏盘', '而且需要人工', '断更新词典', '效率', '达观数据', '机器学习', '方法智能识别', '种变形变换', '内容', '同时根据最新', '样本数据实时更新运算模型', '自动学习更新', '保证检测', '效果', '实时', '弹幕能够', '处理', '达观数据文本挖掘系统支持高并发大数据量实时处理', '完全', '支持实时弹幕', '处理', '实现', '弹幕文本', '筛除涉黄', '涉政', '垃圾评论', '广告内容', '检测', '标签自动提取', '非热门行业适用', '达观标签自动提取功能', '利用行业数据', '模型训练和调整', '在接入', '非热门行业服务', '行业', '规范文本', '训练样本', '模型训练', '模型更新', '适应此行业', '个性化需求', '而且在后期应用', '过程模型', '更新迭代保证提取', '结果与行业', '发展保持同步']\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "文本挖掘主要有哪些功能\n",
    "达观数据拥有多年的自然语言处理技术经验，掌握从词语短串到篇章分析各层面的分析技术，在此基础之上提供以下文本挖掘功能：\n",
    "* 涉黄涉政检测：对文本内容做涉黄涉政检测，满足相应政策要求；\n",
    "* 垃圾评论过滤：在论坛发言或用户评论中，过滤文本中的垃圾广告，提升文本总体质量；\n",
    "* 情感分析：对用户评论等文本内容做情感分析，指导决策与运营；\n",
    "* 自动标签提取：自动提取文本重要内容生成关键性标签，在此基础之上拓展更多功能形式；\n",
    "* 文本自动分类：通过对文本内容进行分析，给出文本所属的类别和置信度，支持二级分类。\n",
    "正常政治言论也会被过滤掉吗？\n",
    "不会，达观对涉政内容会返回一个“反动”权值，取值范围0到1。当涉政内容的反动权值接近“1”时，文本的反动倾向很高，根据客户要求可以直接过滤掉，当反动权值接近“0”时，则文本为正常政治言论的几率就非常高，客户可通过反动权值控制审核松紧程度。\n",
    "黄反内容、垃圾广告形式多样怎么处理？\n",
    "传统的方法更多的是通过配词典的方式来解决。但是这种方法遇到变形文本时命中率很低，造成严重的漏盘，而且需要人工不断更新词典，效率很低。\n",
    "达观数据通过机器学习的方法智能识别各种变形变换的内容，同时根据最新的样本数据实时更新运算模型，自动学习更新，保证检测的效果。\n",
    "实时的弹幕能够做处理吗？\n",
    "可以，达观数据文本挖掘系统支持高并发大数据量实时处理，完全可以支持实时弹幕的处理，实现对弹幕文本做筛除涉黄、涉政、垃圾评论、广告内容等的检测。\n",
    "标签自动提取对于非热门行业适用吗？\n",
    "达观标签自动提取功能可以利用行业数据进行模型训练和调整，在接入一个非热门行业服务之前，我们会以此行业的规范文本作为训练样本做模型训练，新的模型更新之后会适应此行业的个性化需求，而且在后期应用的过程模型会不断的更新迭代保证提取的结果与行业的发展保持同步。'''\n",
    "\n",
    "for r in ['主要有','那些','哪些','\\n','或','拥有','。','，','之前','以下','对于','；','：','、','会','我们','在此','之上',\n",
    "          '*','各','从''而且','一个','以此','作为','之后','当','进行','？','怎么','更多','可以',\n",
    "          '不','通过','吗', '也','可','但是','这种','遇到','则','就','对','等','很','做','中的','的'\n",
    "         ]:\n",
    "    text=text.replace(r,'DELIM')\n",
    "print(text)\n",
    "words=[t.strip() for t in text.split('DELIM')]\n",
    "print(words)\n",
    "words_filtered=list(filter(lambda a:a not in [''] and len(a)>1,words))\n",
    "\n",
    "print(words_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 如何移除文本中的停用词 ?\n",
    "\n",
    "Q. 从文本中移除类似“我们”、“，”、“想要”、“一个”这样的无意义词汇和标点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'达观 数据 客户 意见 洞察 平台 公司 品牌 进行 负面 舆情 实时 监测 事件 演变 趋势 预测 预警 实时 触达 帮助 公司 市场 品牌 部门 第一 时间 发现 负面 舆情 及时 应对 危机 公关 控制 舆论 走向 防止 品牌 受损'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import nltk\n",
    "text = \"达观数据客户意见洞察平台对公司品牌进行负面舆情实时监测，事件演变趋势预测，预警实时触达，帮助公司市场及品牌部门第一时间发现负面舆情，及时应对危机公关，控制舆论走向，防止品牌受损。\"\n",
    "my_stopwords =  [i.strip() for i in open('data/stopword_cn.txt',encoding='utf-8').readlines()]\n",
    "new_tokens=[]\n",
    "# Tokenization using word_tokenize()\n",
    "all_tokens=jieba.lcut(text)\n",
    "for token in all_tokens:\n",
    "    if token in all_tokens:\n",
    "        if token not in my_stopwords:\n",
    "            new_tokens.append(token)\n",
    "\n",
    "\" \".join(new_tokens)            \n",
    "\n",
    "\n",
    "# nltk.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 如何进行词干化（stemming）？\n",
    "\n",
    "\n",
    "Q. 在给定的文本中，将每个词条（token）转换为它的词根形式（root form）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“ 哇，这个新来的boy真的sup handsom ! ” , “ 这个timetable做的not veri good ” , “ coffe 我们需要meet一下然后看看tomorrow怎么安排 ” ,'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming with nltk's PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "stemmer_tokens=[]\n",
    "\n",
    "text= '''\n",
    " “哇，这个新来的boy真的super handsome!”,\n",
    " “这个timetable做的not very good”,\n",
    " “coffee 我们需要meet一下然后看看tomorrow怎么安排”,'''\n",
    "\n",
    "for token in nltk.word_tokenize(text):\n",
    "    stemmer_tokens.append(stemmer.stem(token))\n",
    "    \n",
    "\" \".join(stemmer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 如何从邮箱号中抽取出姓名 ?\n",
    "\n",
    "Q. 利用正则从邮箱地址中抽取出姓名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Scottish_folds_meow']\n"
     ]
    }
   ],
   "source": [
    "# Using regular expression to extract usernames\n",
    "import re\n",
    "\n",
    "text = '我的邮箱号是Scottish_folds_meow@gmail.com。'\n",
    "usernames=re.findall('([^\\u4E00-\\u9FA5]\\w.*)@',text,re.M|re.I)\n",
    "print(usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 如何在排除停用词的情况下找出文中最常见的词汇？\n",
    "\n",
    "\n",
    "Q. 在启用停用词的情况下，抽取给定文本段落中的TOP10高频词汇。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['文本', '挖掘', '主要', '功能', '达观', '数据', '拥有', '多年', '自然语言', '处理', '技术', '经验', '掌握', '词语', '短串', '篇章', '分析', '层面', '分析', '技术', '此基础', '之上', '提供', '以下', '文本', '挖掘', '功能', '涉黄', '涉政', '检测', '文本', '内容', '做涉', '黄涉政', '检测', '满足', '相应', '政策', '要求', '垃圾', '评论', '过滤', '论坛', '发言', '用户', '评论', '过滤', '文本', '垃圾', '广告', '提升', '文本', '总体', '质量', '情感', '分析', '用户', '评论', '文本', '内容', '情感', '分析', '指导', '决策', '运营', '自动', '标签', '提取', '自动', '提取', '文本', '重要', '内容', '生成', '关键性', '标签', '此基础', '之上', '拓展', '多功能', '形式', '文本', '自动', '分类', '文本', '内容', '进行', '分析', '给出', '文本', '所属', '类别', '置信度', '支持', '二级', '分类', '正常', '政治', '言论', '过滤', '不会', '达观', '涉政', '内容', '返回', '一个', '反动', '权值', '取值', '范围', '当涉政', '内容', '反动', '权值', '接近', '文本', '反动', '倾向', '客户', '要求', '直接', '过滤', '反动', '权值', '接近', '文本', '正常', '政治', '言论', '几率', '非常', '客户', '反动', '权值', '控制', '审核', '松紧', '程度', '黄反', '内容', '垃圾', '广告', '形式多样', '处理', '传统', '方法', '词典', '方式', '解决', '这种', '方法', '遇到', '变形', '文本', '命中率', '造成', '严重', '漏盘', '需要', '人工', '不断更新', '词典', '效率', '达观', '数据', '机器', '学习', '方法', '智能', '识别', '变形', '变换', '内容', '最新', '样本', '数据', '实时', '更新', '运算', '模型', '自动', '学习', '更新', '保证', '检测', '效果', '实时', '弹幕', '能够', '处理', '达观', '数据', '文本', '挖掘', '系统', '支持', '并发', '数据量', '实时处理', '完全', '支持', '实时', '弹幕', '处理', '实现', '弹幕', '文本', '筛除', '涉黄', '涉政', '垃圾', '评论', '广告', '内容', '检测', '标签', '自动', '提取', '热门', '行业', '适用', '达观', '标签', '自动', '提取', '功能', '利用', '行业', '数据', '进行', '模型', '训练', '调整', '接入', '一个', '热门', '行业', '服务', '之前', '以此', '行业', '规范', '文本', '训练样本', '模型', '训练', '模型', '更新', '之后', '适应', '行业', '个性化', '需求', '后期', '应用', '过程', '模型', '不断', '更新', '迭代', '保证', '提取', '行业', '发展', '保持', '同步']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('文本', 16),\n",
       " ('内容', 9),\n",
       " ('自动', 6),\n",
       " ('行业', 6),\n",
       " ('达观', 5),\n",
       " ('数据', 5),\n",
       " ('分析', 5),\n",
       " ('提取', 5),\n",
       " ('反动', 5),\n",
       " ('模型', 5),\n",
       " ('处理', 4),\n",
       " ('检测', 4),\n",
       " ('垃圾', 4),\n",
       " ('评论', 4),\n",
       " ('过滤', 4),\n",
       " ('标签', 4),\n",
       " ('权值', 4),\n",
       " ('更新', 4),\n",
       " ('挖掘', 3),\n",
       " ('功能', 3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "text='''\n",
    "文本挖掘主要有哪些功能\n",
    "达观数据拥有多年的自然语言处理技术经验，掌握从词语短串到篇章分析个层面的分析技术，在此基础之上提供以下文本挖掘功能：\n",
    "* 涉黄涉政检测：对文本内容做涉黄涉政检测，满足相应政策要求；\n",
    "* 垃圾评论过滤：在论坛发言或用户评论中，过滤文本中的垃圾广告，提升文本总体质量；\n",
    "* 情感分析：对用户评论等文本内容做情感分析，指导决策与运营；\n",
    "* 自动标签提取：自动提取文本重要内容生成关键性标签，在此基础之上拓展更多功能形式；\n",
    "* 文本自动分类：通过对文本内容进行分析，给出文本所属的类别和置信度，支持二级分类。\n",
    "正常政治言论也会被过滤掉吗？\n",
    "不会，达观对涉政内容会返回一个“反动”权值，取值范围0到1。当涉政内容的反动权值接近“1”时，文本的反动倾向很高，根据客户要求可以直接过滤掉，当反动权值接近“0”时，则文本为正常政治言论的几率就非常高，客户可通过反动权值控制审核松紧程度。\n",
    "黄反内容、垃圾广告形式多样怎么处理？\n",
    "传统的方法更多的是通过配词典的方式来解决。但是这种方法遇到变形文本时命中率很低，造成严重的漏盘，而且需要人工不断更新词典，效率很低。\n",
    "达观数据通过机器学习的方法智能识别各种变形变换的内容，同时根据最新的样本数据实时更新运算模型，自动学习更新，保证检测的效果。\n",
    "实时的弹幕能够做处理吗？\n",
    "可以，达观数据文本挖掘系统支持高并发大数据量实时处理，完全可以支持实时弹幕的处理，实现对弹幕文本做筛除涉黄、涉政、垃圾评论、广告内容等的检测。\n",
    "标签自动提取对于非热门行业适用吗？\n",
    "达观标签自动提取功能可以利用行业数据进行模型训练和调整，在接入一个非热门行业服务之前，我们会以此行业的规范文本作为训练样本做模型训练，新的模型更新之后会适应此行业的个性化需求，而且在后期应用的过程模型会不断的更新迭代保证提取的结果与行业的发展保持同步。'''\n",
    "\n",
    "my_stopwords =  [i.strip() for i in open('data/stopword_cn.txt',encoding='utf-8').readlines()]\n",
    "new_tokens=[]\n",
    "# Tokenization using word_tokenize()\n",
    "all_tokens=jieba.lcut(text.strip(' ').replace('\\n',''))\n",
    "\n",
    "for token in all_tokens:\n",
    "    if len(token)>1:#仅关注词长度大于1的词汇\n",
    "        if token not in my_stopwords:\n",
    "            new_tokens.append(token)\n",
    "print(new_tokens)#查看一下结果\n",
    "freq_dict={}\n",
    "\n",
    "#Calculating frequency count\n",
    "for word in new_tokens:\n",
    "    if word not in freq_dict:\n",
    "        freq_dict[word]=1\n",
    "    else:\n",
    "        freq_dict[word]+=1\n",
    "\n",
    "        \n",
    "sorted(freq_dict.items(),key=lambda x:x[1],reverse=True)[:20]#按照键进行降序排列，仅展示TOP20高频词汇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. 如何对给定文本进行拼写纠错 ?\n",
    "\n",
    "Q. 通过训练具体领域的语言模型来纠正该领域文本中的拼写错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建一个车辆评论的txt文档\n",
    "import pandas as pd\n",
    "data=pd.read_csv('data/train_2.csv')\n",
    "data=data['content']\n",
    "with open('data/car_reviews.txt','w',encoding='utf8') as f:\n",
    "    for i in range(len(data)):\n",
    "        f.writelines(data[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,collections\n",
    "\n",
    "def words(text):\n",
    "    return re.findall('[\\u4e00-\\u9fa5_a-zA-Z]+',text.lower())\n",
    "\n",
    "def train(features):\n",
    "    model=collections.defaultdict(lambda:1)\n",
    "    for f in features:\n",
    "        model[f]+=1\n",
    "    return model\n",
    "\n",
    "NWORDS=train(words(open('data/car_reviews.txt',encoding='utf8').read()))\n",
    "\n",
    "hanzi = open('data/hanzi.txt',encoding='utf-8').read()\n",
    "\n",
    "def edits1(word):\n",
    "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "   deletes    = [a + b[1:] for a, b in splits if b]\n",
    "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "   replaces   = [a + c + b[1:] for a, b in splits for c in hanzi if b]\n",
    "   inserts    = [a + c + b   for a, b in splits for c in hanzi]\n",
    "   return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words): \n",
    "    return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'后备箱'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct(\"后箱备\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b2dbe90c2c46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"油蚝量\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-207f8f356ace>\u001b[0m in \u001b[0;36mcorrect\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0medits1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mknown_edits2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNWORDS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-207f8f356ace>\u001b[0m in \u001b[0;36mknown_edits2\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mknown_edits2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mNWORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-207f8f356ace>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mknown_edits2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mNWORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# correct(\"油蚝量\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. 如何度量若干文本之间的余弦相似度（cosine similarity）?\n",
    "\n",
    "Q. 用sklearn中的文本特征抽取器CountVectorizer和TfidfVectorizer来实现文本相似度计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Vectorizer of sklearn to get vector representation\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "text1 = '涉黄涉政检测：对文本内容做涉黄涉政检测，满足相应政策要求'\n",
    "text2 = '垃圾评论过滤：在论坛发言或用户评论中，过滤文本中的垃圾广告，提升文本总体质量'\n",
    "text3 = '情感分析：对用户评论等文本内容做情感分析，指导决策与运营'\n",
    "text4 = '自动标签提取：自动提取文本重要内容生成关键性标签，在此基础之上拓展更多功能形式'\n",
    "text5 = '文本自动分类：通过对文本内容进行分析，给出文本所属的类别和置信度，支持二级分类。'\n",
    "\n",
    "documents=[text1,text2,text3,text4,text5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************\n",
      "vectorizer\n",
      " CountVectorizer(stop_words=['$', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
      "                            '9', '?', '_', '“', '”', '、', '。', '《', '》', '一',\n",
      "                            '一些', '一何', '一切', '一则', '一方面', '一旦', '一来', '一样',\n",
      "                            '一般', '一转眼', ...],\n",
      "                tokenizer=<function <lambda> at 0x00000182C840A438>)\n",
      "******************\n",
      "matrix\n",
      "   (0, 48)\t4\n",
      "  (0, 71)\t2\n",
      "  (0, 0)\t14\n",
      "  (0, 40)\t3\n",
      "  (0, 45)\t2\n",
      "  (0, 47)\t2\n",
      "  (0, 41)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 49)\t1\n",
      "  (0, 64)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 54)\t1\n",
      "  (0, 46)\t1\n",
      "  (1, 0)\t22\n",
      "  (1, 41)\t2\n",
      "  (1, 18)\t2\n",
      "  (1, 16)\t2\n",
      "  (1, 62)\t2\n",
      "  (1, 50)\t2\n",
      "  (1, 17)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 61)\t1\n",
      "  (1, 34)\t1\n",
      "  :\t:\n",
      "  (3, 35)\t1\n",
      "  (3, 22)\t1\n",
      "  (3, 42)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 28)\t1\n",
      "  (3, 27)\t1\n",
      "  (4, 0)\t22\n",
      "  (4, 41)\t3\n",
      "  (4, 20)\t1\n",
      "  (4, 9)\t3\n",
      "  (4, 43)\t1\n",
      "  (4, 11)\t1\n",
      "  (4, 56)\t3\n",
      "  (4, 67)\t1\n",
      "  (4, 66)\t1\n",
      "  (4, 60)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 23)\t1\n",
      "  (4, 58)\t1\n",
      "  (4, 4)\t1\n",
      "  (4, 26)\t1\n",
      "  (4, 39)\t1\n",
      "  (4, 36)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 57)\t1\n",
      "******************\n",
      "doc_term_matrix\n",
      " [[14  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "   0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  1  0  0  0  2  1  2\n",
      "   4  1  0  0  1  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  2]\n",
      " [22  2  0  1  0  0  0  0  0  0  0  0  1  1  0  1  2  1  2  0  0  0  0  0\n",
      "   1  0  0  0  0  0  1  0  0  0  1  0  0  0  1  0  0  2  0  0  0  0  0  0\n",
      "   0  0  2  0  0  0  0  0  0  0  0  0  0  1  2  1  0  0  0  0  0  1  0  0]\n",
      " [16  0  0  0  0  1  0  1  0  2  0  0  0  0  0  0  0  0  0  0  1  1  0  0\n",
      "   0  0  0  0  0  0  0  2  2  0  1  0  0  1  0  0  0  1  0  2  0  0  0  0\n",
      "   0  0  0  0  0  0  1  0  0  0  0  1  0  0  1  0  0  1  0  0  0  0  0  0]\n",
      " [19  0  0  0  0  0  1  0  0  0  1  2  0  0  2  0  0  0  0  1  1  0  1  0\n",
      "   0  0  0  1  1  1  0  0  0  1  0  1  0  0  2  0  0  1  1  0  2  0  0  0\n",
      "   0  0  0  1  0  1  0  2  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0]\n",
      " [22  0  1  0  1  0  0  0  1  3  0  1  0  0  0  0  0  0  0  0  1  0  0  1\n",
      "   0  0  1  0  0  0  0  0  0  0  0  0  1  0  0  1  0  3  0  1  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  3  1  1  0  1  0  0  0  0  0  1  1  0  0  0  0]]\n",
      "[[1.         0.87388108 0.87123226 0.8646331  0.87448837]\n",
      " [0.87388108 1.         0.93062202 0.92878571 0.9369179 ]\n",
      " [0.87123226 0.93062202 1.         0.91292049 0.94344219]\n",
      " [0.8646331  0.92878571 0.91292049 1.         0.92784991]\n",
      " [0.87448837 0.9369179  0.94344219 0.92784991 1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [' ', '万', '两', '中', '乌', '二', '云', '什', '仅', '介', '代', '令', '会', '似', '位', '体', '余', '作', '例', '便', '候', '值', '假', '傥', '先', '光', '免', '全', '关', '具', '兼', '况', '出', '分', '切', '前', '办', '加', '单', '反', '受', '句', '叮', '否', '呼', '哒', '哧', '唯', '唷', '啪', '啷', '喔', '喻', '固', '基', '处', '外', '天', '夫', '奈', '妨', '始', '孰', '家', '少', '尚', '岂', '巧', '年', '幸', '庶', '开', '徒', '循', '怕', '总', '恰', '悉', '惟', '愿', '慢', '成', '截', '抑', '拘', '换', '接', '料', '方', '旁', '旦', '旧', '时', '曰', '期', '极', '果', '样', '根', '次', '止', '正', '步', '死', '毋', '没', '消', '漫', '点', '然', '特', '犹', '独', '甚', '登', '直', '相', '省', '真', '眨', '眼', '知', '确', '种', '竟', '简', '算', '类', '紧', '结', '继', '综', '罢', '肯', '般', '莫', '见', '言', '譬', '许', '设', '诚', '话', '说', '贼', '赖', '越', '身', '转', '达', '进', '述', '逐', '通', '道', '遵', '部', '鄙', '里', '鉴', '针', '问', '间', '限', '难', '非', '面', '首', '齐'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(\n",
    "    stop_words=my_stopwords,\n",
    "    tokenizer=lambda x:' '.join(jieba.lcut(x))\n",
    ")\n",
    "print(\"******************\")\n",
    "print(\"vectorizer\\n\",vectorizer)\n",
    "print(\"******************\")\n",
    "matrix=vectorizer.fit_transform(documents)\n",
    "print(\"matrix\\n\",matrix)\n",
    "# Obtaining the document-word matrix\n",
    "doc_term_matrix=matrix.todense()\n",
    "\n",
    "print(\"******************\")\n",
    "print(\"doc_term_matrix\\n\",doc_term_matrix)\n",
    "# Computing cosine similarity\n",
    "df = pd.DataFrame(doc_term_matrix)\n",
    "\n",
    "print(cosine_similarity(df,df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.63129661 0.62897379 0.60798811 0.63684255]\n",
      " [0.63129661 1.         0.77851542 0.76417751 0.79151747]\n",
      " [0.62897379 0.77851542 1.         0.73176078 0.8152873 ]\n",
      " [0.60798811 0.76417751 0.73176078 1.         0.76928226]\n",
      " [0.63684255 0.79151747 0.8152873  0.76928226 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#使用tfidf检测\n",
    "vecorizer = TfidfVectorizer(\n",
    "    stop_words= my_stopwords,\n",
    "    tokenizer =lambda x : ' '.join(jieba.lcut(x)))\n",
    "\n",
    "\n",
    "\n",
    "matrix=vectorizer.fit_transform(documents)\n",
    "\n",
    "# Obtaining the document-word matrix\n",
    "doc_term_matrix=matrix.todense()\n",
    "doc_term_matrix\n",
    "\n",
    "# Computing cosine similarity\n",
    "df = pd.DataFrame(doc_term_matrix)\n",
    "\n",
    "\n",
    "print(cosine_similarity(df,df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. 如何计算文本间的soft cosine similarity（\"软性\"余弦值） ?\n",
    "\n",
    "Q. 利用gensim中的计算文档间的\"软性\"余弦值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import jieba\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.similarities import termsim\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '涉黄涉政检测：对文本内容做涉黄涉政检测，满足相应政策要求'\n",
    "text2 = '垃圾评论过滤：在论坛发言或用户评论中，过滤文本中的垃圾广告，提升文本总体质量'\n",
    "text3 = '情感分析：对用户评论等文本内容做情感分析，指导决策与运营'\n",
    "text4 = '自动标签提取：自动提取文本重要内容生成关键性标签，在此基础之上拓展更多功能形式'\n",
    "text5 = '文本自动分类：通过对文本内容进行分析，给出文本所属的类别和置信度，支持二级分类。'\n",
    "\n",
    "documents=[text1,text2,text3,text4,text5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#检测编码\n",
    "import chardet\n",
    "with open('C:/Users/Eric/Desktop/github实现有趣的代码/Word-Embedding-master/word2vec/toutiao_word_embedding.bin','rb') as f:\n",
    "    bytes=f.read()\n",
    "\n",
    "chardet.detect(bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xad in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3f42e1387f17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Eric/Desktop/github实现有趣的代码/Word-Embedding-master/word2vec/toutiao_word_embedding.bin'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    380\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34mb'\\n'\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# ignore newlines in front of words (some binary files have)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m                         \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_deprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                     \u001b[1;31m# TODO use frombuffer or something similar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xad in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "model=KeyedVectors.load_word2vec_format('C:/Users/Eric/Desktop/github实现有趣的代码/Word-Embedding-master/word2vec/toutiao_word_embedding.bin',binary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
