{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句子情感分类\n",
    "\n",
    "我们的目标是创建一个模型，该模型以句子为输入（就如上述数据集中的评论），输出为1（句子带有积极情感）或者0（句子带有消极情感）。\n",
    "实际上，该模型是由两个子模型组成的。\n",
    "\n",
    "DistilBERT模型负责处理句子并将从中提取的信息传给下一个模型，这是由HuggingFace团队开发的开源知识蒸馏版BERT模型，量级更轻、运行更快，性能堪比BERT。\n",
    "\n",
    "下一个模型是scikit-learn中一个基本的逻辑回归模型，接收DistilBERT处理的结果，并将句子分类为积极或消极（分别为1或0）。\n",
    "\n",
    "两个模型间传递的数据是一个768维的向量。我们可以把这个向量当作用于分类的句子的嵌入（embedding）。\n",
    "\n",
    "## 模型训练\n",
    "虽然我们要用到两个模型，但只需训练逻辑回归模型即可。DistillBERT模型将使用适用于英语语言处理的预训练模型。这种模型没有专门为句子分类任务进行过训练和微调，但是，基于BERT模型的通用目标，它还是具有一定的句子分类能力，尤其是第一位置（与[CLS]标志相关）的BERT输出。我认为，这是由BERT模型的次要训练目标，即下一句分类（Next sentence classification）决定的。这一目标似乎是训练模型去封装整句意思作为第一位置的输出。Transformers库包含DistilBERT模型及其预训练版本模型的实现。\n",
    "\n",
    "讲解的网址：\n",
    "https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb\n",
    "\n",
    "https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247492910&idx=4&sn=4b884e7a72bd5c231f0244274efacb1c&chksm=ebb7ddfadcc054ec4121ed0d5d48b2d14aa9810ebfe3d0d4353f9b3a2b896f9b611f4686e29e&mpshare=1&scene=24&srcid=0418PcAneZb84Lz46CjEFx1a&sharer_sharetime=1587216489039&sharer_shareid=1176345ce55a5e0024df723ca29d3249&key=240d40e83da786d0b11b0fa5f82466301828c99323aa10856d86cb6a8e186878312ed2ef020da3f9997ce589e597bb4f98823056e1b567a43efd9ac3698d5cc30a64df90d379765343f888c3a8592cebeee4def25bded5d82a4db18906f2af1e8a5bbd9f0fde293cf1dc3e2aa187d388c6340d0b86c6989ac08703b80d932c6a&ascene=14&uin=MzM4MTIwMjg2MA%3D%3D&devicetype=Windows+10+x64&version=6300002f&lang=zh_CN&exportkey=A8h%2FCbBMOVM%2FfgZxORy8PZg%3D&pass_ticket=Jk9za4U5mb%2FrD6y79f0yBI%2B2K6ty5t0bYcfkqx0Ev3dHEBcJ74BIdfW00BCTppUR&wx_header=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# 首先，使用训练后的distilBERT模型来生成数据集中2000个句子的句子嵌入。\n",
    "# 这一步后就不再用distilBERT，剩下的都是scikit-learn的工作。依照惯例，将数据集划分为训练集和测试集。\n",
    "# 第一步，使用BERT 分词器将英文单词转化为标准词（token）。第二步，加上句子分类所需的特殊标准词（special token，如在首位的[CLS]和句子结尾的[SEP]）。\n",
    "# 第三步，分词器会用嵌入表中的id替换每一个标准词（嵌入表是从训练好的模型中得到的），词嵌入的背景知识可参见我的《图解Word2Vec》。\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb#pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学习直接从github中导入数据集\n",
    "#翻墙之后下载下来的成功率更高\n",
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  a stirring , funny and finally transporting re...  1\n",
       "1  apparently reassembled from the cutting room f...  0\n",
       "2  they presume their audience wo n't sit still f...  0\n",
       "3  this is a visually stunning rumination on love...  1\n",
       "4  jonathan parker 's bartleby should have been t...  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入预训练的DistilBERT模型和分词器\n",
    "model_class,tokenizer_class,pretrained_weights=(ppb.DistilBertModel,ppb.DistilBertTokenizer,'distilbert-base-uncased')\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 现在我们可以对数据集进行分词。请注意，此处的操作与上面的示例有些不同。上面的示例仅进行了一个句子的分词。在这里，我们将分批处理所有句子的分词（考虑到资源问题，ipython notebook文件中只处理了一小部分数据，约2000个）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "6915    [101, 9145, 1010, 7570, 18752, 14116, 1998, 28...\n",
       "6916    [101, 2202, 2729, 2003, 19957, 2864, 2011, 103...\n",
       "6917    [101, 1996, 5896, 4472, 4121, 1010, 3082, 7832...\n",
       "6918    [101, 1037, 5667, 2919, 2143, 2007, 5667, 2561...\n",
       "6919    [101, 1037, 12090, 2135, 2512, 5054, 19570, 23...\n",
       "Name: 0, Length: 6920, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词\n",
    "tokenized=df[0].apply((lambda x:tokenizer.encode(x,add_special_tokens=True)))\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(tokenized[10])\n",
    "# tokenized\n",
    "np.array(tokenized).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding步骤\n",
    "max_len=0\n",
    "for i in tokenized.values:\n",
    "    if len(i)>max_len:\n",
    "        max_len=len(i)\n",
    "padded=np.array([i+[0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Masking步骤\n",
    "#DistilBert无此操作\n",
    "# 如果直接将padded送过去，可能会造成混淆。\n",
    "# 我们需要创造另一个参数变量来告诉他忽略mask以及我们增加的padding\n",
    "attention_mask=np.where(padded!=0,1,0)\n",
    "attention_mask.shape\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#接下来我们就可以进行深度学习了\n",
    "input_ids=torch.LongTensor(padded)\n",
    "# attention_mask=torch.tensor(np.array(attention_mask))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states=model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行此步骤后，last_hidden_states保存DistilBERT的输出。它是一个具有多维度的元组（示例个数，序列中的最大符号的个数，DistilBERT模型中的隐藏单元数）。在我们的例子中是2000（因为我们自行限制为2000个示例），66（这是2000个示例中最长序列中的词数量），768（DistilBERT模型中的隐藏单位数量）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重要部分切片\n",
    "# 对于句子分类问题，我们仅对[CLS]标记的BERT输出感兴趣，\n",
    "# 因此我们只选择该三维数据集的一个切片。\n",
    "feature=last_hidden_states[0][:,0,:].numpy()\n",
    "# 现在我们获得了features这个二维numpy数组，它包含数据集中所有句子的句子嵌入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic回归数据集\n",
    "### 现在我们有了BERT的输出，已经具备训练逻辑回归模型所需的完整数据集。768列数据是特征集，而标签可以从初始数据集中获得。\n",
    "\n",
    "我们用来训练Logistic回归的标记数据集。其中，特征是上图中切片得到的[CLS]标记（位置0）的BERT输出向量。每行对应于我们数据集中的一个句子，每列对应于Bert / DistilBERT模型顶部转换器（transformer）中前馈神经网络的隐藏单元的输出。\n",
    "\n",
    "在完成传统的机器学习训练集、测试集划分之后，我们得到了最终的逻辑回归模型并可以对数据集进行训练了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=df[1]\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(feature, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练模型\n",
    "lr_clf=LogisticRegression()\n",
    "lr_clf.fit(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8341040462427746"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#得到训练的模型之后进行测试评估\n",
    "lr_clf.score(test_features,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
